## 제2장 기계번역을 정복한 인공지능

#### 너무 어려운 인간의 언어

> `기계번역(Machine Translation)`  
> 1949년 학술 논문을 통해 처음 등장한 용어로, 인간이 사용하는 자연어를 기계를 이용해 다른 언어로 번역해내는 일을 뜻함

- 인간의 언어는 너무 어려워서 기계번역은 오랜 시간동안 상용화되지 못했다.
  1. 너무 많은 규칙 : 인간의 언어는 일정한 규칙에 체계적으로 따라 발전하지 않는다. 역사와 유행에 따라 무작위로 변하며 이전에 쓰던 말이 사라지기도, 신조어가 생기면서 확장되기도 한다.
  2. 너무 많은 오류 : 일상적인 대화에도 문법적 오류는 많다. 그럼에도 우리의 뇌가 웬만한 오류를 스스로 보정하고 이해하기 때문에 대화가 가능한 것이다.
  3. 너무 많은 의미 : 같은 발음의 단어가 여러 뜻을 같는 경우에, 규칙을 일일이 정의해서 매번 상황에 맞게 번역하는 것은 어렵다.

#### 인공 신경망, 언어에 도전하다

- 인공 신경망을 도입한 번역은 놀라운 성과를 냈다. 기존에는 규칙에 따라 번역하거나 단어별로 먼저 번역하고 문장을 조합하는 과정을 거쳤으나, 인공 신경망을 도입하면서 이러한 과정을 모두 생략하고 문장을 통째로 번역해냈다.
- 인공 신경망의 번역 과정 : 문장을 통째로 숫자 형태의 `벡터(Vector)`로 압축 - 압축한 값을 번역할 언어로 옮김 - 벡터 값을 계산해 가장 확률이 높은 단어를 차례로 찾아내면서 번역문 생성

#### 트랜스포머 모델, 언어를 이해하다

> `버트(BERT; Bidirectional Encoder Representations from Transformers)`  
> 2018년 가을, 구글에서 개발한 트랜스포머 응용 모델. 인코더 구조를 기반으로 하여 언어를 이해하는 데 탁월한 모델이다.

- 최초 공개 당시, 버트는 구글에서 개발됐기 때문에 구글에서 공개한 딥러닝 프레임워크인 텐서플로(TensorFlow)에서만 동작했다.
- 하지만 그 당시는 메타에서 공개한 파이토치(PyTorch)가 인기를 끌었고, 허깅페이스라는 회사에서 버트를 파이토치에서 동작하도록 개조했다.
- 그 당시 허깅페이스는 막 시작하던 스타트업이었지만 버트 이후 GPT까지 연동하면서 지금은 한화로 기업가치가 6조원이 넘는 기업으로 급격히 성장했다.

---

#### Comment

- (p.56-67) 전작에서도 인코더, 디코터, 어텐션 등 인공 신경망 번역의 동작 원리에 대한 서술과 관련 일러스트들이 이해하기 어렵게 작성되었다고 느꼈는데 개선되지는 않았다.
- 제2장은 전작과 거의 중복되는 내용이라 빠르게 읽었다. 어쩔 수는 없겠지만 있겠지만, 전작을 읽고 넘어오는 독자들에 대한 배려가 조금 부족하지 않나 생각했다.
  - 이에 2장은 간단히 정리하고, 앞서 작성한 AI for Beginners의 제6장으로 갈음한다.
