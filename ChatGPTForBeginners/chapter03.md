## 제3장 챗GPT를 완성한 비밀 레시피

```
< Discussion >
최근 오픈AI에서 환각(hallucination)의 구조적 원인에 대해 정리한 논문을 발표했습니다.

☑️ 정리하면, 대부분 언어 모델의 평가 체계가 추측을 보상하는 방식으로 설계되었기 때문에 환각이 발생하는데, 구체적으로는 모른다고 하거나 아무 말도 하지 않기보다 뭐라도 말을 하는 것이 유리한 구조이기 때문이라고 합니다.

그리고 이러한 평가 체계를 '불확실성 표명 자체에 보상을 주는' 식으로 바꾸어 개선하면 할루시네이션을 줄일 수 있다고 주장합니다.

☑️ 여기서 궁금했던 점은,
그렇다면 언어 모델을 훈련시키는 과정에서, 모델 스스로가 질문에 대한 답을 미리 알고 있었는지, 아닌지에 대한 여부를 알아야 (그래야 강화학습도 가능하므로) 보상도 그에 맞게 변경할 수 있는 것이 아닌가?

그러나 다음에 나올 단어를 예측하는 것을 기반으로 하는 언어 모델에서 이 같은 부분을 어떻게 스스로 인지할 수 있는가 하는 점이었습니다.

또한 정답이 없는 문제에 대한 보상은 어떻게 처리할 것인지, 명확한 정답이 있는 문제와 그렇지 않은 문제에 대한 보상은 어떻게 구분할 수 있을지도 궁금했습니다.

☑️ 북클럽 시간에는 아래의 내용에 대해 이야기 해보고 싶습니다.

오픈AI는 이 논문에서 LLM이 시험을 보는 학생과 같다고 했습니다. 실제로 정답이 있는 문제를 푸는 학생의 입장에서는 답을 내기를 포기하는 것보다 틀린 답이라도 시도해보는 게 현명한 선택이겠죠. 또는 면접을 보는 지원자, 혁신을 추구하는 창업가라면 틀리더라도 시도해보아야 합니다.

그치만 정확한 사실만을 기반으로 답해야 하는 전문가라면? (예를 들어 의사) 혹은 불확실한 명제를 연구하는 연구자라면? 모른다고 말하는 것이 훨씬 책임있는 자세겠죠. 이처럼 상황과 역할에 따라 다른 태도가 요구됩니다.

사회적으로는 어떤 것 같은가요? 사회가 이런 각각의 상황에 맞는 평가와 보상을 주고 있다고 생각하시나요? 그리고 이를 바탕으로 AI의 성능을 높이기 위해서는 어떤 기준을 적용해서 보상 체계를 계선할 수 있을까요?
...

[원문]
https://openai.com/ko-KR/index/why-language-models-hallucinate/

https://arxiv.org/abs/2509.04664

[참고]
https://velog.io/@qlgks1/why-language-models-hallucinate
```

#### 세상을 바꿀 GPT 역사의 시작

> `강화학습(Reinforcement Learning)`  
> 기계가 스스로 학습하며 성능을 향상시키는 방식. 강화학습의 대표적인 결과물로 알파고가 있다.
>
> `매개변수(Parameter)`  
> 모델의 학습 과정에서 조정되는 가중치 값들

#### 임베딩과 토큰, 언어를 효율적으로 쪼개는 방법

> `벡터(Vector)`  
> 텍스트 데이터를 컴퓨터가 이해하고 처리할 수 있도록 수학적으로 표현한 숫자 배열
>
> `임베딩(Embedding)`  
> 단어나 문장을 숫자 벡터로 표현하는 과정. 임베딩은 '끼워 넣다'는 뜻인데, 단어의 의미나 문맥, 관계 등의 정보를 다차원 실수(Real Number) 공간에 끼워 넣는 과정을 의미함
>
> `토큰(Token)`  
> 언어 모델에서 학습을 진행하는 최소 단위. 대개 단어와 비슷한 의미이나, 반드시 일치하지는 않는다. 주로 수많은 문장을 읽어들인 다음 이에 대한 통계적인 결과로 구성된다. 토큰으로 만드는 과정은 `토크나이징(Tokenizing)`, 토크나이징하는 도구는 `토크나이저(Tokenizer)`라고 한다.

- 임베딩 벡터는 수많은 단어를 학습하면서 여러 의미를 담아 자동으로 만들어내는 값이다.
  - 오픈AI에서 제공하는 임베딩 API는 3,072차원이다. 각각의 단어가 3,072개의 특징을 지닌 숫자를 갖고 있는 셈
- 임베딩이 무작정 크다고 항상 좋은 것은 아니다. 단어의 특징을 제대로 표현하기 위해 100차원 정도라도 값이 적절히 분배될 수 있는 최적의 임베딩이 훨씬 더 중요하다.
- 정리하면, 토크나이저를 이용해 문장을 토큰 단위로 토크나이저 하고, 각 토큰의 임베딩 벡터를 가져와 언어 모델에 보내 학습을 진행하는 것이다.

#### 어텐션, GPT의 핵심 알고리즘

- 어텐션의 원리는 중요한 단어에 가중치를 주는 개념이다.
- GPT는 어텐션만으로 구성된 트랜스포머 모델의 디코더 방식을 채택했다.
- GPT가 채택한 트랜스포머 구조에서 어텐션은 입력값을 먼저 Q(Query), K(Key), V(Value) 세 개의 값으로 나눈다.
  - Q와 K 사이의 유사도를 계산하고 가장 유사한 부분에 가중치를 높여 V와 계산한 값이 바로 결과가 된다.
- GPT는 동일한 문장에서 서로 간의 관계를 찾는다. 즉 찾고자 하는 정보 Q와 정보 목록인 K가 동일하다.
  - 같은 문장에서 서로의 관계를 찾기 때문에 이를 `셀프 어텐션`이라고 한다.

> `어텐션맵(Attention Map)`  
> 데이터의 크기를 색의 강도로 나타내어 어느 부분에 집중해야 하는지 보여주는 히트맵 형태의 구조. 각 토큰 간에 관계의 강도를 나타내는 히트맵이다. 세로축은 Q, 가로축은 K로, QK 행렬이다.

- QK 어텐션맵을 V와 결합하여 최종 결과를 만들어낸다.
- GPT 계열의 모델은 다음에 나올 토큰을 예측하는 생성 모델인데 아직 등장하지도 않은 단어를 참조한다는 것은 논리에 맞지 않다. 따라서 뒤에 나오는 토큰은 참조하지 않도록 가려준다. 이 과정을 `마스킹(Masking)`이라 하는데 어텐션맵 상에서 우측 상단 절반 정도를 모두 가려서 값을 하나도 부여하지 않는다.
  - 셀프 어텐션은 이를 반영하여 전체 이름을 `마스크드 셀프 어텐션(Masked Self Attention)`이라 한다.
- 어텐션을 한 개만 사용하는 것이 아니라, 여러 개 사용한다. 이를 `멀티 헤드 어텐션(Multi-Head Attention)`이라 한다.
- GPT-3는 미래 토큰을 마스킹하고, 임베딩을 96등분으로 나눈 128개를, 96번씩 동시에 어텐션 계산한다. 이를 `마스크드 멀티 헤드 셀프 어텐션(Masked Multi-Head Self-Attention)`이라 한다.
  - GPT-3는 토큰 하나의 임베딩이 12,288차원이다. (96 x 128 = 12,288)
- 그리고 이 과정을 95회 더 반복한다. 멀티 헤드 어텐션으로 96번씩 동시에 계산하고, 이 과정을 총 96회 반복하는 것이다.
  - 즉, GPT-3는 토큰 하나를 예측하기 위해 96 x 96 = 9,216번 어텐션을 진행한다.
- 이처럼 토큰과 토큰 사이의 관계를 수많은 어텐션을 통해 찾아내는 것이 바로 트랜스포머 모델의 핵심이다.

#### 스케일링 법칙, 크면 클수록 좋다

> `클라이버 법칙(Kleiber's Law)`  
> 크기가 커질수록 에너지 효율이 좋아진다는 법칙. 1930년대 초 농생물학자인 막스 클라이버(Max Kleiber)가 발견했기 때문에 그의 이름을 땄다. 이 법칙은 딥러닝 분야에도 동일하게 적용된다.
>
> `신경망 스케일링 법칙(Neural Scaling Law)`  
> 2020년 오픈AI가 도출한 법칙으로, 모델 크기와 데이터 양, 학습 비용(계산량)이 적절히 증가하면 모델 성능도 이에 비례해 개선된다는 것이 핵심. 즉, "크면 클수록 좋다"는 이른바 규모의 법칙으로, 이는 초거대 언어 모델의 등장을 알리는 신호탄이 되었다.

- 지금까지의 연구 결과로는 모델이 크면 클수록 훨씬 더 많은 내용을 풍부하게 이해할 것이라고 추측한다.
  - 실제로 오픈AI가 GPT-2 모델을 공개할 당시 실험 결과를 보면, 모델이 클수록 손실(Loss)\* 값이 줄어드는 것을 확인할 수 있다.
    > \* `손실(Loss)`  
    > 모델의 예측이 실제 정답과 얼마나 다른지를 나타내는 수치. 낮은 손실값은 모델의 예측이 정확하다는 것을 의미함
- 복잡한 문제일수록 좋은 알고리즘을 찾아서 문제를 해결하기보다는, 복잡성을 인정하고 거대한 데이터의 힘을 활용해 문제를 해결하는 게 훨씬 합리적이다.
- 2022년에는 구글 딥마인드가 후속 연구를 통해, 모델의 성능을 1% 높이기 위해서 얼마나 더 큰 모델을 사용해야 하고, 더 많은 학습 데이터를 투입해야 하며, 더 좋은 시스템을 사용해야 하는지 실험을 통해 구체적인 값을 제시했다.
  - 이제 스케일링 법칙은 딥러닝 모델의 성능을 예측하고, 제한된 자원으로 최적의 모델을 설계하는 데 핵심적인 지표가 되었다.

#### RLHF, 챗GPT를 완성하는 비밀 레시피

> `사전 학습(Pre-Training)`  
> 모델이 특정 작업을 수행하기에 앞서, 방대한 양의 일반적인 데이터(라벨 없는 데이터)를 사용해 기본적인 언어 능력이나 패턴을 먼저 학습하는 과정. 별도의 수작업 없이도 대규모 데이터를 손쉽게 구축하고 학습에 활용할 수 있다는 장점이 있다. 이렇게 사전 학습을 통해 만들어진 모델을 사전 학습 모델(PLM; Pre-Trained Language Model)이라 한다.
>
> `지시 모델(Instruct Model)`  
> 사람이 요청한 프롬프트를 잘 따르도록 사전 학습 모델을 개선한 모델
>
> `미세 조정(Fine Tuning)`  
> 사전 학습된 모델을 특정 작업이나 도메인에 맞게 추가로 학습시키는 과정
>
> `사후 학습(Post-Training)`  
> 사전 학습을 거친 모델이 사용자 프롬프트를 잘 따르도록 만드는 과정

- ChatGPT는 사용자 프롬프트를 따르는 미세 조정을 3단계에 걸쳐 진행했다.
  1. 데이터셋을 구축하고 `지도 미세 조정(SFT; Supervised Fine-Tuning)` 모델을 학습한다.
     - 이렇게 만든 모델을 `SFT 모델`이라 한다. 인간이 지도한 내용으로 미세하게 조정한 모델이라는 뜻
     - 즉, 기존 사전 학습 모델에 인간이 세심하게 정제한 데이터를 넣고 더 다듬는다.
  2. SFT 모델에 강화학습을 도입한다.
     - `인간 피드백을 이용한 강화학습(RLHF; Reinforcement Learning from Human Feedback)`이라는 기법을 도입했는데, 여기에는 보상 함수로 근접 정책 최적화(PPO; Proximal Policy Optimization)라는 알고리즘을 사용했다.
     - RLHF를 위한 2단계는 비교 데이터를 구축하고 `보상 모델(Reward Model)`을 학습하는 단계다. RM 모델은 하나의 질문에 여러 답변을 두고 어떤 답변이 만족스러운지 순위를 매기는 과정을 거친다.
  3. 강화학습을 이용해 성능을 높인다. RM 모델을 이용해 보상을 최적화 하는 것
     - 오픈AI는 2017년, 게임에 적용하기 위해 직접 개발했던 `근접 정책 최적화(PPO; Proximal Policy Optimization)` 알고리즘을 언어에 적용했다.

---

#### Comment

- (p.81-88) GPT-1에서 GPT-4에 이어지는 과정과 매개변수의 확대에 대한 내용은 1장에서도 했었고.., 전작부터 하면 3번은 읽었다.
- (p.125-126) 마지막 페이지에 잠깐 언급되는 '활용도가 높은 프롬프트 엔지니어링'과 관련한 내용은 곧 이 책에서 얻고자 하는 인사이트과 연결되어서 5장이 기대가 된다.
- 3장의 전반은 전작에서도 있었던 내용이라 반복 학습하는 느낌으로 읽었다. 다시 읽으니 더 이해가는 내용도 있고, 그새 까먹어서 또 새로운 내용도 있고.
